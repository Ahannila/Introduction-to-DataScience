{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDuUFp-HYZ2b"
   },
   "source": [
    "# Introduction to Data Science 2025\n",
    "\n",
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82jDDoETYZ2g"
   },
   "source": [
    "In this week's exercise, we look at prompting and zero- and few-shot task settings. Below is a text generation example from https://github.com/TurkuNLP/intro-to-nlp/blob/master/text_generation_pipeline_example.ipynb demonstrating how to load a text generation pipeline with a pre-trained model and generate text with a given prompt. Your task is to load a similar pre-trained generative model and assess whether the model succeeds at a set of tasks in zero-shot, one-shot, and two-shot settings.\n",
    "\n",
    "**Note: Downloading and running the pre-trained model locally may take some time. Alternatively, you can open and run this notebook on [Google Colab](https://colab.research.google.com/), as assumed in the following example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIQ1s96UCcJW"
   },
   "source": [
    "## Text generation example\n",
    "\n",
    "This is a brief example of how to run text generation with a causal language model and `pipeline`.\n",
    "\n",
    "Install [transformers](https://huggingface.co/docs/transformers/index) python package. This will be used to load the model and tokenizer and to run generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fUBJmXHCHw-"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZRNZgRJCt6Q"
   },
   "source": [
    "Import the `AutoTokenizer`, `AutoModelForCausalLM`, and `pipeline` classes. The first two support loading tokenizers and generative models from the [Hugging Face repository](https://huggingface.co/models), and the last wraps a tokenizer and a model for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwyK005xCFSF"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QJPDe3ZC_sL"
   },
   "source": [
    "Load a generative model and its tokenizer. You can substitute any other generative model name here (e.g. [other TurkuNLP GPT-3 models](https://huggingface.co/models?sort=downloads&search=turkunlp%2Fgpt3)), but note that Colab may have issues running larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "57a81d2cf1fe49fdb9f5c62488b4a13b",
      "1540a4a7a13242e7aa98218926811955",
      "743cb97149ad4471869dde178be3cb5e",
      "50836ddc6d8c443ca2114ed686d8830a",
      "7cf690b875fc4063ad0d3d9b3c741233",
      "09a88e1f29324847b77e6707f4a495c1",
      "0f6908c649984d3ea0e013da9df07254",
      "bc73566e308d4444b70e495aae46f488",
      "f6408b6d24c749eeb1c0d203ae574899",
      "f18c5a331b2c4aac83870742baf42e60",
      "1396528d01e84fabaef99f8bc824c742",
      "1a21c7ce104c4277aeee609be995f228",
      "7baa04dff5ee4e54bb9c24409cbed315",
      "0dcc91e47d484e60865d8e253090b8e0",
      "545e3391afa54441bb5f7ce9644d78a6",
      "eb3a8d7956c74c7093a0d83a6beefdcf",
      "27b37623de5c40348758f1ada94456cd",
      "3a256c2c56f3423e9602ded7ede860cd",
      "adc6bbc4597b4384b73dd516f84f0f5c",
      "2a76ca2831c44dd88170ee9f8ee51831",
      "3348f1fa5cc04a2bac3109fe4a7d33e0",
      "e34af9f6395d496892d97c08ac248378",
      "70d97ad06d2f4f51a5131e1863b53846",
      "94e72aa81cab47c1bb89fe6d724d8e42",
      "78ea51625a904150be3989281194676c",
      "5262e0ed4e78451faec30674b2981510",
      "fd0e1f7c5dfa45438c1720430db97746",
      "e646c7ac48dd48dcb1b5789a930d01a8",
      "d5351a2adf3d4463aa84aebe08d6576f",
      "d3b8f97967f6431b8c276949c098bae9",
      "85f66b9ae4c4424fa61fa3990da99536",
      "fee7cdbbb631420dbd59d0d6098ff611",
      "bba3823c51e541eb913215f63f8a64b7",
      "620bf362d3c54fcebe5bc1f14d5b00ad",
      "584e1c1de5c14931bf8f11a1372df7fd",
      "4b249a3025a146c3bd4c60d3b79ef71a",
      "9abe4aa7b82144a68dc73831a7e73000",
      "7b8cb8a0cad847d08c0fb150ce1c4586",
      "fb710cfcd43b4d71907cb65c5b934418",
      "8d9a58e90b544c64944ad6d779d4bd0b",
      "4dfca8959e564acabe79d0855c62d536",
      "5562482e772443b18b0bdd167827ec57",
      "bedec6f436ab4a79a4e5d4e0b9203e2e",
      "1305f48552b44cadaa1d897e444ff06a",
      "ca7452cdba1d4504993619c56f230378",
      "869cdd1224aa49888f148b4e7a0513f8",
      "67db5d29708e4632bc4ead201179c8d4",
      "607693d0dbb545f49ed20f18dd7349e6",
      "62a1a168828848e2af0c817b37ea08d2",
      "3c1c44a6d70249629be7074d50666c85",
      "87a30dd8c192409bb97d85fa7f760aac",
      "d7dc7d7273be49948386888411a72ce4",
      "37d3cfd255a949d78702fcf5b0a233de",
      "dbef17180ca243f8bd73f33311974671",
      "9219719b999b4ecf98bd496eab3a9302",
      "661231e8999349cea28d6772af005774",
      "8691c56155004d85a20b6fcc3f4b284f",
      "d0a05c625af84b38b51509be4e10b3b5",
      "db6906e07fb44a268fe09c3b189a6040",
      "e304aefe0f59448a93482c774cfc3870",
      "3d555e7411c2473287ef991a59232578",
      "838326bb0a75475e9c9411b4c51590f6",
      "9444fd4910034f8cb42615466888861c",
      "67c480d833d14e8484e6825cdf405cc8",
      "45a6cf6f95d9449ca80fe44c95bb3717",
      "5d5fe697e7da4629bdb07818a20a7433"
     ]
    },
    "id": "wqTxn_QaCNjZ",
    "outputId": "a70fc08b-0410-4a6a-b3c1-dcc2e870df46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a81d2cf1fe49fdb9f5c62488b4a13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/218 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a21c7ce104c4277aeee609be995f228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d97ad06d2f4f51a5131e1863b53846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620bf362d3c54fcebe5bc1f14d5b00ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/562 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7452cdba1d4504993619c56f230378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661231e8999349cea28d6772af005774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = 'TurkuNLP/gpt3-finnish-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ADWWb77e1sY"
   },
   "source": [
    "Instantiate a text generation pipeline using the tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IIJzNrEe5qx",
    "outputId": "7b986498-49e5-4a04-98c5-a3d780e9d0a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAohNr1ciwaU"
   },
   "source": [
    "We can now call the pipeline with a text prompt; it will take care of tokenizing, encoding, generation, and decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "jWcOJkiKi5vr",
    "outputId": "9f8b30e7-3bec-4d45-c4bf-5aee3a9ecb79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Terve, miten menee?”\\n”Loistavasti, kiitos kysymästä.”\\n”Sinulla on ollut vähän vastoinkäymisiä, eikö totta?”\\n”Ei'}]\n"
     ]
    }
   ],
   "source": [
    "output = pipe('Terve, miten menee?', max_new_tokens=25)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNRMsxXOjSo0"
   },
   "source": [
    "Just print the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Op7MJ6XjahG",
    "outputId": "314a9a0d-5ecd-410e-8192-e6201792cd07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terve, miten menee?”\n",
      "”Loistavasti, kiitos kysymästä.”\n",
      "”Sinulla on ollut vähän vastoinkäymisiä, eikö totta?”\n",
      "”Ei\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YROp3hyikXPO"
   },
   "source": [
    "We can also call the pipeline with any arguments that the model `generate` function supports. For details on text generation using `transformers`, see e.g. [this tutorial](https://huggingface.co/blog/how-to-generate).\n",
    "\n",
    "Example with sampling and a high `temperature` parameter to generate more chaotic output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22QjXE88jkim",
    "outputId": "1a7f14c3-df5e-49cb-9b24-9441f2a355fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terve, miten menee? Hei tyttö.\n",
      "»Hyvää kesää.\n",
      "A.\n",
      "Onko hän nyt täällä meillä? »Rakkaan poikaparka. Mutta sinulla ei vielä\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    'Terve, miten menee?',\n",
    "    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hpboa4DGYZ2t"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Your task is to assess whether a generative model succeeds in the following tasks in zero-shot, one-shot, and two-shot settings:\n",
    "\n",
    "- binary sentiment classification (positive / negative)\n",
    "\n",
    "- person name recognition\n",
    "\n",
    "- two-digit addition (e.g. 11 + 22 = 33)\n",
    "\n",
    "For example, for assessing whether a generative model can name capital cities, we could use the following prompts:\n",
    "\n",
    "- zero-shot:\n",
    "\t>\"\"\"\\\n",
    "\t>Identify the capital cities of countries.\n",
    "\t>\n",
    "\t>Question: What is the capital of Finland?\\\n",
    "\t>Answer:\\\n",
    "\t>\"\"\"\n",
    "- one-shot:\n",
    "\t>\"\"\"\\\n",
    "\t>Identify the capital cities of countries.\n",
    "\t>\n",
    "\t>Question: What is the capital of Sweden?\\\n",
    "\t>Answer: Stockholm\n",
    "\t>\n",
    "\t>Question: What is the capital of Finland?\\\n",
    "\t>Answer:\\\n",
    "\t>\"\"\"\n",
    "- two-shot:\n",
    "\t>\"\"\"\\\n",
    "\t>Identify the capital cities of countries.\n",
    "\t>\n",
    "\t>Question: What is the capital of Sweden?\\\n",
    "\t>Answer: Stockholm\n",
    "\t>\n",
    "\t>Question: What is the capital of Denmark?\\\n",
    "\t>Answer: Copenhagen\n",
    "\t>\n",
    "\t>Question: What is the capital of Finland?\\\n",
    "\t>Answer:\\\n",
    "\t>\"\"\"\n",
    "\n",
    "You can do the tasks either in English or Finnish and use a generative model of your choice from the Hugging Face models repository, for example the following models:\n",
    "\n",
    "- English: `gpt2-large`\n",
    "- Finnish: `TurkuNLP/gpt3-finnish-large`\n",
    "\n",
    "You can either come up with your own instructions for the tasks or use the following:\n",
    "\n",
    "- English:\n",
    "\t- binary sentiment classification: \"Do the following texts express a positive or negative sentiment?\"\n",
    "\t- person name recognition: \"List the person names occurring in the following texts.\"\n",
    "\t- two-digit addition: \"This is a first grade math exam.\"\n",
    "- Finnish:\n",
    "\t- binary sentiment classification: \"Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta?\"\n",
    "\t- person name recognition: \"Listaa seuraavissa teksteissä mainitut henkilönnimet.\"\n",
    "\t- two-digit addition: \"Tämä on ensimmäisen luokan matematiikan koe.\"\n",
    "\n",
    "Come up with at least two test cases for each of the three tasks, and come up with your own one- and two-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpFsG4d9e13Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hhPmBPmLYZ2u",
    "outputId": "464f3e58-a356-4f1d-c951-88c89bbb7d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta?\n",
      "\n",
      "Kysymys 1: Rakastin konserttia — ääni oli kirkas.\n",
      "Vastaus:\n",
      "\n",
      "Kysymys 2: Palvelu oli hidasta ja ruoka oli kylmää.\n",
      "Vastaus: -\n",
      "Kysymys kuuluu: Mistä pidän musiikissa on, onko minussa joitakin musiikin herättämisiä toiveita?\n",
      "\n",
      "Vastaus 4 > > Kysymykset 5: 1.)\n"
     ]
    }
   ],
   "source": [
    "# Use this cell for your code\n",
    "\n",
    "output = pipe(\n",
    "       \"Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta?\\n\\nKysymys 1: Rakastin konserttia — ääni oli kirkas.\\nVastaus:\\n\\nKysymys 2: Palvelu oli hidasta ja ruoka oli kylmää.\\nVastaus:\",\n",
    "    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AV4F6v_PYZ2u"
   },
   "source": [
    "**Submit this exercise by submitting your code and your answers to the above questions as comments on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8EMYFI1ieCkI",
    "outputId": "49cbad3e-f7a6-4fef-9478-f7d71b14337d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta?\n",
      "\n",
      "Kysymys 1: Rakastin konserttia — ääni oli kirkas.\n",
      "Vastaus: Positiivista\n",
      "\n",
      "Kysymys 2: Palvelu oli hidasta ja ruoka oli kylmää.\n",
      "Vastaus: Ilmavaa tai äänepainoista tunnetta ilmaisema.\n",
      "\n",
      "Entä löytyykö mielestäsi jotakin asiaa kuvaavaa tunnetilana: jännittynyt (jännitys)?\n",
      "Ilmaa\n"
     ]
    }
   ],
   "source": [
    "# Use this cell for your code\n",
    "\n",
    "output = pipe(\n",
    "       \"Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta?\\n\\nKysymys 1: Rakastin konserttia — ääni oli kirkas.\\nVastaus: Positiivista\\n\\nKysymys 2: Palvelu oli hidasta ja ruoka oli kylmää.\\nVastaus:\",\n",
    "    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5vxPfAAeSav",
    "outputId": "e798392f-d838-47b8-a60c-7d0c4a6a4604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta?\n",
      "\n",
      "Esimerkki 1: Sovellus kaatuu koko ajan ja se turhauttaa minua.\n",
      "Vastaus: Negatiivinen\n",
      "\n",
      "Esimerkki 2: Ihana yllätys — tämä piristi päivääni!\n",
      "Vastaus: Positiivinen\n",
      "\n",
      "Kysymys 1: Rakastin konserttia — ääni oli kirkas.\n",
      "Vastaus:\n",
      "\n",
      "Kysymys 2: Palvelu oli hidasta ja ruoka oli kylmää.\n",
      "Vastaus: Se sai ruokahalulta loppua ja minua vihoittelemaan palvelun hitaauden yhteydessä kokit voisivat palkata apumiehiä ja huolehtia ruuan lämmittämistoi\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta? Esimerkki 1: Sovellus kaatuu koko ajan ja se turhauttaa minua. Vastaus: Negatiivinen Esimerkki 2: Ihana yllätys — tämä piristi päivääni! Vastaus: Positiivinen Kysymys 1: Rakastin konserttia — ääni oli kirkas. Vastaus:Kysymys 2: Palvelu oli hidasta ja ruoka oli kylmää. Vastaus:\",\n",
    "    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "on7M2TsNe2w7",
    "outputId": "1dc93fc7-7a59-4dac-d702-73e4325adfcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listaa seuraavissa teksteissä mainitut henkilönnimet. Teksti 1: Arttu tapasi Tapanin matkallaan, jotta he voivat käydän hengaamassa Marian kanssa. Teksti 2: John oli vanhoollismielinen, koska hänen kasvatuksensa Artturin kanssa oli vanhoollinen. Kun se nyt niin kamalasti teitä kiinnostaa.\n",
      "\n",
      "Kun me ollaan puhuttu ton Marian Antin tapaamisesta mun kanssa ollaan yleensä menty vähän eri reiteillä koska\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Listaa seuraavissa teksteissä mainitut henkilönnimet. Teksti 1: Arttu tapasi Tapanin matkallaan, jotta he voivat käydän hengaamassa Marian kanssa. Teksti 2: John oli vanhoollismielinen, koska hänen kasvatuksensa Artturin kanssa oli vanhoollinen.\",    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxEFD9NxgCFT",
    "outputId": "0b390b29-dd5a-4f5d-f3ca-fa81a4b2880b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listaa seuraavissa teksteissä mainitut henkilönnimet. Teksti 1: Arttu tapasi Tapanin matkallaan, jotta he voivat käydän hengaamassa Marian kanssa. Vastaus: Arttu, Tapani ja Maria Teksti 2: John oli vanhoollismielinen, koska hänen kasvatuksensa Artturin kanssa oli vanhoollinen. Tapani otti asian rennosti.\n",
      "Hän istui koko päiväs aman risaiseen vaat. Ei mikään kovin hyvä homma. Siinä saattoi\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Listaa seuraavissa teksteissä mainitut henkilönnimet. Teksti 1: Arttu tapasi Tapanin matkallaan, jotta he voivat käydän hengaamassa Marian kanssa. Vastaus: Arttu, Tapani ja Maria Teksti 2: John oli vanhoollismielinen, koska hänen kasvatuksensa Artturin kanssa oli vanhoollinen.\",\n",
    "    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omOQspuAhZYd",
    "outputId": "0c6844ac-d8dc-41c2-df8b-d53d12d6f582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listaa seuraavissa teksteissä mainitut henkilönnimet. Teksti 1: Arttu tapasi Tapanin matkallaan, jotta he voivat käydän hengaamassa Marian kanssa. Vastaus: Arttu, Tapani ja Maria Teksti 2: John oli vanhoollismielinen, koska hänen kasvatuksensa Artturin kanssa oli vanhoollinen. Vastaus: John, Artturi. Teksti 3: John kävi pyörällä Painimassa Oliverin kanssa. Vastaus: Alexander pääsi sisäkautta karkaamaan Paisujakselta pihalle kesken illan kello 23:na Joulu yönä eli 24 Joulu p :n\n",
      "19 6\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Listaa seuraavissa teksteissä mainitut henkilönnimet. Teksti 1: Arttu tapasi Tapanin matkallaan, jotta he voivat käydän hengaamassa Marian kanssa. Vastaus: Arttu, Tapani ja Maria Teksti 2: John oli vanhoollismielinen, koska hänen kasvatuksensa Artturin kanssa oli vanhoollinen. Vastaus: John, Artturi. Teksti 3: John kävi pyörällä Painimassa Oliverin kanssa. Vastaus:\",\n",
    "    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "co0hicowiLpS",
    "outputId": "2cc157eb-8daa-47b4-8589-cb7ac50d950f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tämä on ensimmäisen luokan matematiikan koe. Ratkaise kukin yhteenlasku ja palauta vain lopullinen kokonaisluku ilman välivaiheita. Kysymys 1: 47 + 18 = Vastaus: Kysymys 2: 26 + 57 = Vastaus: 7225; Vastaus 14: 48648 Vastaus 4: 561353 Vastauksia kaikkiin tehtäviin tulee neljä joka sivulle, sillä vastaussivun\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Tämä on ensimmäisen luokan matematiikan koe. Ratkaise kukin yhteenlasku ja palauta vain lopullinen kokonaisluku ilman välivaiheita. Kysymys 1: 47 + 18 = Vastaus: Kysymys 2: 26 + 57 = Vastaus:\",\n",
    "    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgOJIu0tig5o",
    "outputId": "6ed17da7-0366-473a-cddd-92fb0bbfa81e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tämä on ensimmäisen luokan matematiikan koe. Ratkaise kukin yhteenlasku ja palauta vain lopullinen kokonaisluku ilman välivaiheita. Kysymys 1: 47 + 18 = 65 Vastaus: Kysymys 2: 26 + 57 = Vastaus: 27.2. (3p+kysymyskohtainen essee)\n",
      "Matemaattinen mallinnusohjelmisto sisältää matematii­seita kaaviointitasoja tietojen\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Tämä on ensimmäisen luokan matematiikan koe. Ratkaise kukin yhteenlasku ja palauta vain lopullinen kokonaisluku ilman välivaiheita. Kysymys 1: 47 + 18 = 65 Vastaus: Kysymys 2: 26 + 57 = Vastaus:\",\n",
    "    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scvlyO4ri-K_",
    "outputId": "c9640ebf-a067-4fab-8db4-9dc85caac811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tämä on ensimmäisen luokan matematiikan koe. Ratkaise kukin yhteenlasku ja palauta vain lopullinen kokonaisluku ilman välivaiheita. Kysymys 1: 47 + 18 = Vastaus: 65 Kysymys 2: 26 + 57 = Vastaus: 83 Kysymys 3: 11 + 22 = Vastaus: ᄄ 0 Tehtävä 1: 67 x 5+9*271x308 + 277= 861 Ei vastausta 3\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    \"Tämä on ensimmäisen luokan matematiikan koe. Ratkaise kukin yhteenlasku ja palauta vain lopullinen kokonaisluku ilman välivaiheita. Kysymys 1: 47 + 18 = Vastaus: 65 Kysymys 2: 26 + 57 = Vastaus: 83 Kysymys 3: 11 + 22 = Vastaus: \",\n",
    "    do_sample=True,\n",
    "    temperature=10.0,\n",
    "    max_new_tokens=25\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
